---
title: "Retrieve and Process Climate Centroid Data"
author: "Caitlin Mothes and Katie Willi"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    theme: paper
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      error = FALSE,
                      message = FALSE)
                      #cache = TRUE)

source("setup.R")
```

# Workflow to process MACA climate centroid data

**Codebase modified from <https://github.com/nationalparkservice/CCRP_automated_climate_futures>, led by Amber Runyon.**

Function to pull in all park centroid files:

```{r}
get_files <- function(park) {
  walk(list.files(
    paste0("data/park/", park, "/centroid/climate"),
    full.names = TRUE
  ),
  function(x) {
    tmp <- read_csv(x)
    # hacky way to pull filename to assign to env object
    name <-  str_sub(x, 33,-5)
    assign(name, tmp, envir = .GlobalEnv)
  })
}



```

## Get data for park:

```{r}
park <- "BRCA"

## get all files
get_files("BRCA")

## create list of future and historic dfs
## MAKE SURE no other objects with '_future' or '_historical' in their names
future_dfs <- mget(ls(pattern = "_future"))

historic_dfs <- mget(ls(pattern = "_historical"))

```

## Set up parameters

```{r}
CFs_all <- c("Warm Wet", "Hot Wet", "Central", "Warm Dry", "Hot Dry")

##Color schemes

#Colors for CF values plotted side by side (match order of CFs vector)
colors5 <-  c("#6EB2D4", "#05689F", "#F6B294", "#CA0020","grey")
colors5.2 <- c("#6EB2D4", "#05689F", "grey", "#F6B294", "#CA0020")

# Switch for using Tercek csvs or downloading own data:
centroids_csv <- "Y" 
# GCM selection method one of "corners" or  "pca"
method <- "pca" 
# Percentage of models to drop from ranking:
Percent_skill_cutoff = .1
# Indicates whether Q/I present at bottom of plot to ID CF method used:
MethodCaption = "Y" 


# Threshold percentages for defining Climate futures. Default low/high:  0.25, 0.75
CFLow = 0.25     
CFHigh = 0.75

# Quantiles for temperature threshold calculations
QuantileLow = 0.05
QuantileHigh = 0.95

HotTemp = 95
ColdTemp = 32
PrecipThreshold = 0.05
```

### Variable calculations:

```{r}
TFtoC <- function(T){(T-32)/1.8}

# VP from FAO -  https://www.fao.org/3/x0490e/x0490e07.htm
# could also use Buck 1981 for 'improved':
# Buck: VPDsat (mb) = (1.0007 + (3.46 * 10^-6 * P)) * 6.1121 * exp((17.50 * T)/(T+240.87))
# where T is deg C and P is atm pressure mb. Above for P > 800 (correction is minimal)
# Zackman re: Ragwala uses: Es = 611.6441 * 10^[(7.591386*T)/(240.7263+T)] where Tavg.
# Shelley sent Vaisala- to use Tavg for 611.6441 parameter - for 020 to +50 C.
# VPsatT = saturation VP @ T deg C [kPa]
# VPD [kPa]
VPsatT <- function(T){0.6108 * exp((17.27 * T)/(T + 237.3))}   

VPD <- function(TminF, TmaxF, RHmin, RHmax){
  Tmin <- TFtoC(TminF); Tmax <- TFtoC(TmaxF)
  es <- (VPsatT(Tmin)+VPsatT(Tmax))/2
  ea <- (VPsatT(Tmin)*RHmax*.01 + VPsatT(Tmax)*RHmin*.01)/2
  es - ea   }  # end VPD  
```

## Clean data

Following methods in draft Climate report from Dave, "Methods for assessing climate change exposure for national park planning"; Runyon et al. 2023.

```{r}
# future data, filter to 2035-2065 (2050 mean)
future_all <- future_dfs[[1]] %>%
  dplyr::rename(precip_in = `Precip (in)`,
         tmin_f = `Tmin (F)`,
         tmax_f = `Tmax (F)`,
         rhmax = `RHmax (%)`,
         rhmin = `RHmin(%)`,
         tavg_f = `Tavg (F)`) %>% 
  mutate(
    year = format(Date, "%Y"),
    VPD = VPD(tmin_f, tmax_f, rhmin, rhmax), # do we need vapor pressure??
    DOY = yday(Date)
  ) %>% 
  filter(year %in% 2035:2065)
  

# historic data, filter to 1979-2012 baseline
historic_all <- historic_dfs[[1]] %>%
 dplyr::rename(precip_in = `Precip (in)`,
         tmin_f = `Tmin (F)`,
         tmax_f = `Tmax (F)`,
         rhmax = `RHmax (%)`,
         rhmin = `RHmin(%)`,
         tavg_f = `Tavg (F)`) %>% 
  mutate(
    year = format(Date, "%Y"),
    VPD = VPD(tmin_f, tmax_f, rhmin, rhmax),
    DOY = yday(Date)
  ) %>% 
  filter(year %in% 1979:2012)
```

## Low skill models

```{r}
# Determine low-skill models using list created from Rupp et al. 2016 (this is from the CCRP team)

# assign region of park (one of SWR, SER, PWR or 'mean' if none of these)
region <- "SWR"


low_skill_models <-
  read_delim('data/GCM_skill_by_region.txt') %>%
  filter(if (region %in% Region) {
    Region == region
  } else {
    Region == "mean"
  }) %>%
  # remove period at end of GCM names (will need later)
  mutate(GCM = str_sub(GCM, 1, -2)) %>% 
  # Worse models have higher value rank
  slice_max(n = length(unique(future_all$GCM)) / 2 * Percent_skill_cutoff,
            order_by  = Rank)

```

## Calculate Deltas

Calculate all baseline values, averages, and change from baseline to 2050 average.

```{r}

# baseline means from historic data
baseline <- historic_all %>% 
  summarise(baseline_pr = mean(precip_in),
            baseline_tmax = mean(tmax_f),
            baseline_tmin = mean(tmin_f),
            baseline_tavg = mean(tavg_f),
            baseline_rhmax = mean(rhmax),
            baseline_rhmin = mean(rhmin))


# future means for each GCM
future_means <- future_all %>% 
  group_by(GCM) %>% 
  summarise_at(vars(precip_in:tavg_f), mean, na.rm = TRUE) %>% 
  # add delta columns using baseline values
  mutate(delta_pr = precip_in - baseline$baseline_pr,
         delta_tmax = tmax_f - baseline$baseline_tmax,
         delta_tmin = tmin_f - baseline$baseline_tmin,
         delta_tavg = tavg_f - baseline$baseline_tavg,
         delta_rhmax = rhmax - baseline$baseline_rhmax,
         delta_rhmin = rhmin - baseline$baseline_rhmin) %>% 
  # remove low skill models
   separate_wider_delim(GCM, 
                       names = c("GCM_only", "RCP"),
           delim = ".",
           cols_remove  = FALSE) %>% 
  filter(!GCM_only %in% low_skill_models$GCM)
  


```

## Assign Climate Future Categories

```{r}
#### Set limits for CF classification
Pr0 <-  as.numeric(quantile(future_means$delta_pr, 0))
Pr25 <-  as.numeric(quantile(future_means$delta_pr, 0.25))
PrAvg <-  mean(future_means$delta_pr)
Pr75 <-  as.numeric(quantile(future_means$delta_pr, 0.75))
Pr100 <-  as.numeric(quantile(future_means$delta_pr, 1))
Tavg0 <-  as.numeric(quantile(future_means$delta_tavg, 0))
Tavg25 <-  as.numeric(quantile(future_means$delta_tavg, 0.25)) 
Tavg <-  mean(future_means$delta_tavg)
Tavg75 <-  as.numeric(quantile(future_means$delta_tavg, 0.75))
Tavg100 <-  as.numeric(quantile(future_means$delta_tavg, 1))

# CF assignment
future_means <- future_means %>%
  # designate climate future classification based on cf limits
  mutate(
    CF = case_when(
      delta_tavg < Tavg &
        delta_pr > Pr75 |
        delta_tavg < Tavg25 & delta_pr > PrAvg ~ "Warm Wet",
      delta_tavg > Tavg &
        delta_pr > Pr75 |
        delta_tavg > Tavg75 & delta_pr > PrAvg ~ "Hot Wet",
      delta_tavg > Tavg25 &
        delta_tavg < Tavg75 &
        delta_pr > Pr25 & delta_pr < Pr75 ~ "Central",
      delta_tavg < Tavg &
        delta_pr < Pr25 |
        delta_tavg < Tavg25 & delta_pr < PrAvg ~ "Warm Dry",
      delta_tavg > Tavg &
        delta_pr < Pr25 |
        delta_tavg > Tavg75 & delta_pr < PrAvg ~ "Hot Dry"
    )
  )


```

## Corners Method

```{r}
#### Select Corner GCMs, assuming temp on x and precip on y
lx = min(future_means$delta_tavg)
ux = max(future_means$delta_tavg)
ly = min(future_means$delta_pr)
uy = max(future_means$delta_pr)

  #convert to points
ww = c(lx,uy)
wd = c(lx,ly)
hw = c(ux,uy)
hd = c(ux,ly)

corners <- future_means %>%
  # calculate euclidiean distance of each point/model from each corner
  mutate(
    ww_dist = sqrt((delta_tavg - ww[1]) ^ 2 + (delta_pr - ww[2]) ^ 2),
    wd_dist = sqrt((delta_tavg - wd[1]) ^ 2 + (delta_pr - wd[2]) ^ 2),
    hw_dist = sqrt((delta_tavg - hw[1]) ^ 2 + (delta_pr - hw[2]) ^ 2),
    hd_dist =  sqrt((delta_tavg - hd[1]) ^ 2 + (delta_pr - hd[2]) ^ 2)
  )


  # assign CF to each selected corner model
future_means <- future_means %>%
  mutate(
    corners = case_when(
      GCM == filter(corners, CF == "Warm Wet") %>% slice(which.min(ww_dist)) %>% .$GCM ~ "Warm Wet",
      GCM == filter(corners, CF == "Warm Dry") %>% slice(which.min(wd_dist)) %>% .$GCM ~ "Warm Dry",
      GCM == filter(corners, CF == "Hot Wet") %>% slice(which.min(hw_dist)) %>% .$GCM ~ "Hot Wet",
      GCM == filter(corners, CF == "Hot Dry") %>% slice(which.min(hd_dist)) %>% .$GCM ~ "Hot Dry"
      
    )
  )

```

## PCA Method

using just change in precip and avg temp (Runyon et al. methods)

```{r}
# set up for PCA
future_pca_1 <- future_means %>% 
  dplyr::select(GCM, delta_pr, delta_tavg) %>% 
  # set up for prcomp
  column_to_rownames(var = 'GCM')


pca_1 <- prcomp(future_pca_1, center = TRUE, scale. = TRUE) 

# quick plot
autoplot(pca_1, data = future_pca_1, loadings = TRUE,label=TRUE)

# get dataframe
pca_1_df <- as.data.frame(pca_1$x)


#Take the min/max of each of the PCs
PCs <- pca_1_df %>% 
 filter(PC1 == min(PC1) |
        PC1 == max(PC1) |
        PC2 == min(PC2) |
        PC2 == max(PC2)) %>% 
  rownames_to_column(var = "GCM")



#Assigns CFs to diagonals
diagonals <-
  rbind(
    data.frame(CF = CFs_all[c(1, 5)], diagonals = factor("diagonal1")),
    data.frame(CF = CFs_all[c(4, 2)], diagonals = factor("diagonal2"))
  )


PCA <-
  future_means %>% filter(GCM %in% PCs$GCM) %>% left_join(diagonals, by = "CF") %>% right_join(PCs, by = "GCM")

# create column with selected pca models
future_means <- future_means %>%
  mutate(pca = if_else(GCM %in% PCs$GCM,
                       CF,
                       NA))



```

Handling missing and/or extra quadrats

```{r}

# function to deal with redundant quadrat
ID.redundant.gcm <- function(PCA){
  redundant.diag = count(PCA, diagonals)$diagonals[which(count(PCA, diagonals)$n ==
                                                           1)] #ID redundant diagonal
  PC.foul = PCA$PC[which(PCA$diagonals == redundant.diag)] #ID which PC has the redundant diagonal
  PCA$GCM[which(PCA$PC == PC.foul &
                  PCA$GCM != PCA$GCM[which(PCA$diagonals == redundant.diag)])] #ID GCM that is in both the  redundant diagonal and the duplicative PC
}


if(length(setdiff(CFs_all[CFs_all != "Central"], future_means$pca)) > 0) {
  #if a quadrant is missing
  Future_Means$pca[which(Future_Means$corners == setdiff(CFs_all[CFs_all != "Central"], Future_Means$pca))] = setdiff(CFs_all[CFs_all != "Central"], Future_Means$pca) #assign corners selection to that CF
  if (nrow(PCA[duplicated(PCA$GCM), ]) > 0) {
    #If there is a redundant GCM
    Future_Means$pca = Future_Means$pca #Do nothing - otherwise end up with empty quadrant. This line could be removed and make the previous statment inverse but it makes it more confusing what's gonig on that way
  } else{
    Future_Means$pca[which(Future_Means$GCM == ID.redundant.gcm(PCA))] = NA #Removes the GCM that is in redundant diagonal
  }
}



```

## Return Selected Models

```{r}
# return selected methods based on method identified in parameters
selected_gcms <- future_means %>% 
  drop_na(method) %>% 
  select(GCM, CF)
```

### Figures 

From <https://github.com/nationalparkservice/CCRP_automated_climate_futures/blob/master/scripts/Scatter_and_diagnostic.R>

```{r}
library(ggrepel)

Longx<- "annual average temperature (F)"
Longy<- "annual average precipitation (in)"
x <- "DeltaTavg"
y <- "DeltaPr"


# No color
dualscatter = ggplot(future_means, aes(delta_tavg, delta_pr*365, xmin=Tavg25, xmax=Tavg75, ymin=Pr25*365, ymax=Pr75*365))

dualscatter  + geom_text_repel(aes(label=GCM)) +
  geom_point(colour="black",size=4) +
  theme(axis.text=element_text(size=16),
        axis.title.x=element_text(size=16,vjust=-0.2),
        axis.title.y=element_text(size=16,vjust=0.2),
        plot.title=element_text(size=20,face="bold",vjust=2,hjust=0.5),
        legend.text=element_text(size=16), legend.title=element_text(size=16)) + 
  ###
  labs(title =paste(park," Changes in climate means in ", 2050, " by GCM run",sep=""), 
       x = paste("Changes in ",Longx,sep=""), # Change
       y = paste("Changes in ",Longy,sep="")) + #change
  scale_color_manual(name="Scenarios", values=c("black")) +
  # scale_fill_manual(name="Scenarios",values = c("black")) + 
  theme(legend.position="none") +
  geom_rect(color = "black", alpha=0) + 
  geom_hline(aes(yintercept=mean(delta_pr*365)),linetype=2) + #change
  geom_vline(aes(xintercept=mean(delta_tavg)),linetype=2) 
```

## **Running notes for arguments that would go into a function:**

-   park

-   region (three letter NPS region of park)

-   future year range (min, max)

-   historic year range (min, max; note 1979 is min)

-   percent low skill cut off (default from Amber's code is 10%)

-   selection method
